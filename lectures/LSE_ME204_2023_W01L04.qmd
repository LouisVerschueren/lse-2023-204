---
title: "LSE ME204 Week 01 Day 02 - Lecture Material (Student Version)"
author: "Dr. Jon Cardoso-Silva"
date: "2023-07-13"
date-format: "DD MMMM YYYY" # See https://quarto.org/docs/reference/dates.html#using-a-date-format
format: 
    html:
        link-external-icon: true
        toc: true
---

# ‚öôÔ∏è Setup

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```

Note: if the above throws an error, it's likely because you don't have the `rvest` package installed. You can install it by running the following code:

``` r
install.packages("rvest")
```

# Step 1. üïµüèª Inspect the website

We will be scraping [Books to Scrape](http://books.toscrape.com/), a website that was created for the sole purpose of practising web scraping. All of the prices, products and ratings are fictitious.

To start, open the website in your browser and inspect the HTML code. You can do this by right-clicking on the page and selecting "Inspect" (or a similar option depending on your browser).

Now, let's suppose our goal is to collect data from the homepage and create a <mark>[tidy dataframe](https://lse-dsi.github.io/ME204/2023/weeks/week01/day02/slides.html?q=tidy%20data#/section-1)</mark> with the following columns:

-   `title`: the full title of the book
-   `price`: the price of the book
-   `rating`: the rating of the book

**üë• IN GROUPS OF 3-4:**

Using what you learned in the previous lecture, work on the following task:

-   In the space below, write down the HTML tag(s) and/or HTML attributes you would use to select the necessary data. Be as specific as possible with your answers.

``` markdown
# TITLE 

html -> body -> div -> div  -> div(.row) -> div -> section -> div -> ol -> li -> article -> h3 -> a(title)


# PRICE

html -> body -> div container fluid page -> div page inner -> div -> section -> div -> ol -> li -> article -> div -> p 

# RATING

html -> body -> div -> div  -> div(.row) -> div -> section -> div -> ol -> li -> article -> p
```

# Step 2. ‚õèÔ∏è Scrape the first book

Now let me show you how to use `rvest` to navigate step by step through the following HTML elements that you found above:

## 2.1. Read the HTML code

First, we need to use the `read_html()` function to read the HTML code of the website:

```{r}
url <- "http://books.toscrape.com/"
html <- rvest::read_html(url)
html
```

Does the output look familiar?

## 2.2. Navigate through the HTML elements

Now, we can use the `html_elements()` function to navigate through the HTML elements. This function takes two arguments:

-   `x`: the HTML code
-   `css`: the CSS selector or tag name

**Let's start with `body`**

```{r}
html_elements(x = html, css = "body")
```

or, simply:

```{r}
html_elements(html, "body")
```

or even a third alternative, using the pipe operator:

```{r}
html %>% html_elements("body")
```

As usual, the first thing to do when you don't know what a function does is to check the documentation. You can do this by running `?html_elements` in the console, or even by browsing the [rvest documentation](https://rvest.tidyverse.org/reference/html_elements.html) online.

**What's *immediately* inside `body`?** (Emphasis on *immediately*)

```{r}
html %>% html_elements("body") %>% html_children()
```

**How to go one level deeper?**

Just add a space to the CSS selector:

```{r}
html %>% html_elements("body div") #all the elements of the type div that are children of (or that are inside) body
```

**But what if I want just the first `<div>` element?**

You can use another, similarly named function: `html_element()`. This function is similar to `html_elements()`, but it only returns the first element that matches the CSS selector.

```{r}
html %>% html_element("body div")
```

**How to access elements with a specific class?**

```{r}
html %>% html_elements(".row")
```

üéØ **ACTION POINT**

Now, apply what you've learned above and navigate through the HTML elements until you find the **title** of *the first* book. Work in small groups of 3-4 people to collaborate and share your findings. I will give you 15 minutes to do this and then I'll share the solution.

```{r}
html_elements(x = html, css = "#default > div > div > div > div > section > div:nth-child(2) > ol > li:nth-child(1) > article > h3 > a") %>% html_attr("title")

# or

html %>% html_element("h3 a") %>% html_attr("title")

# or to get all titles

html %>% html_elements("h3 a") %>% html_attr("title")

```

üéØ **ACTION POINT**

Can you find out equally simple ways to get the **price** and **rating** of the first book? Work in small groups of 3-4 people to collaborate and share your findings. I will give you 10 minutes to do this and then I'll share the solution.

```{r}
# price
book_price <- html %>% html_element(".price_color") %>% html_text()
book_price
# rating
book_rating <- html %>% html_element(".star-rating") %>% html_attr("class")
book_rating
book_rating <- word(book_rating, 2)
book_rating
```

# Step 3. ‚õèÔ∏è Scrape all books on the homepage

**üë®üèª‚Äçüè´ TEACHING MOMENT**

Now that we know how to scrape the first book, it is easier to adapt to scrape all books on the homepage. We just need to use the `html_elements()` function instead of `html_element()`.

```{r}
#title
book_titles <- html %>% html_elements("h3 a") %>% html_attr("title")
# price
book_prices <- html %>% html_elements(".price_color") %>% html_text()
book_prices
# rating
book_ratings <- html %>% html_elements(".star-rating") %>% html_attr("class")
book_ratings <- word(book_ratings, 2)
book_ratings
```

üéØ **ACTION POINT**

How would you create a data frame with the title, price and rating of all books on the homepage? Try to work out a solution in 5 minutes and then I'll share the solution.

```{r}


df <- data.frame(title = book_titles, price = book_prices, rating = book_ratings)
df
```

# Step 4. üì¶ Create a function

**üë®üèª‚Äçüè´ TEACHING MOMENT**

Now, let's create a function `scrape_books` that takes a URL as an input and returns a dataframe with the title, price and rating of all books on that page.

```{r}
scrape_books <- function(url) {
  
  # Read the HTML code
  html <- read_html(url)
  
  # Scrape the title
  book_titles <- 
    html %>% 
    html_elements("h3 a") %>% 
    html_attr("title")
  
  # Scrape the price
  book_prices <- 
    html %>% 
    html_elements("p.price_color") %>% 
    html_text()
  
  # Scrape the rating
  book_ratings <- 
    html %>% 
    html_elements("p.star-rating") %>% 
    html_attr("class") %>% 
    word(2)
  
  # Create a dataframe
  df <- tibble(
    title = book_titles,
    price = book_prices,
    rating = book_ratings
  )
  
  # Return the dataframe
  return(df)
}
```

Let's test our function:

```{r}
scrape_books("http://books.toscrape.com/")
```

Test that it works for other subpages that have the same structure:

```{r}
scrape_books("http://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html")
```

# Step 5: üë• Group Exercise

Now, let's put your hyperlink navigation skills to the test!

When you click on a book cover, it will take you to a webpage with additional information about the book. For example, click [here](http://books.toscrape.com/catalogue/the-red-tent_273/index.html) to view the page for the book "The Red Tent".

üéØ **ACTION POINT** (45-60 min)

In your groups, modify the `scrape_books()` function to extract additional information and include it in the final data frame. The new columns to include are:

-   `description`: the description of the book
-   `category`: the category of the book
-   All the columns under "Product Information" (e.g., `UPC`, `Product Type`, `Price (excl. tax)`, etc.)
-   The URL of the book cover image

```{r}

#### Part 1: Try the scrape the required data for just a single book ####
urls_books <- html %>% html_elements("h3 a") %>% html_attr("href")
urls <- paste("http://books.toscrape.com/", urls_books, sep="")

# Read the HTML code of book1 and scrape that page
url_book1 <- urls[1]
html_book1 <- read_html(url_book1)

# get the description of book 1
description_book1 <- html_book1 %>% html_elements("p") %>% html_text()

# get the category of book 1
category_book1 <- as.list(html_book1 %>% html_elements("a") %>% html_text())[4]

# get all the column values under "product information" of book 1
html_book1 %>% html_elements(".table") %>% html_text()


table <- html_book1 %>% html_nodes(xpath = '//*[@id="content_inner"]/article/table') %>% html_table()

product_information_book1 <- select(data.frame(table), "X2")
product_information_book1

# get the image URL of book1
img_url_part2 <- html_book1 %>% html_elements("img") %>% html_attr("src")
img_url_part2
img_url_part2 <- img_url_part2 %>% substr(7, 10000) # choose random high value for end string
img_url_part2
img_url_book1 <- paste("http://books.toscrape.com/", img_url_part2, sep="")
img_url_book1


# put in dataframe
df_book1 <- data.frame(
  description = description_book1, 
  category = category_book1, 
  UPC = product_information_book1[[1]][1],
  product_type = product_information_book1[[1]][2]
  
)





#### Part 2: Make changes to the scrape_books() function ####
scrape_books <- function(url) {
  
  # Read the HTML code
  html <- read_html(url)
  
  # Scrape the title
  book_titles <- 
    html %>% 
    html_elements("h3 a") %>% 
    html_attr("title")
  
  # Scrape the price
  book_prices <- 
    html %>% 
    html_elements("p.price_color") %>% 
    html_text()
  
  # Scrape the rating
  book_ratings <- 
    html %>% 
    html_elements("p.star-rating") %>% 
    html_attr("class") %>% 
    word(2)
  
  # Create a dataframe
  df <- tibble(
    title = book_titles,
    price = book_prices,
    rating = book_ratings
  )
  
  # Add columns to the data frame as placeholders for the new data
  namevector <- sapply(table, "[", "X1")
  namevector <- unlist(namevector)
  df[ , c("Description", "Category", "URL book cover image", c(namevector)) ] <- NA
  
  # Fill in the newly added columns for each book using a loop
  urls_books <- html %>% html_elements("h3 a") %>% html_attr("href")
  urls <- paste("http://books.toscrape.com/", urls_books, sep="")
  
  for(i in 1:20) {
    # Read the HTML code of book i and scrape that page
    url_booki <- urls[i]
    html_booki <- read_html(url_booki)
    
    # get the description of book i
    description_booki <- as.list(html_booki %>% html_elements("p") %>% html_text())[4]
    
    # get the category of book i
    category_booki <- as.list(html_booki %>% html_elements("a") %>% html_text())[4]
    
    # get all the column values under "product information" of book i
    html_booki %>% html_elements(".table") %>% html_text()
    table <- html_booki %>% html_nodes(xpath = '//*[@id="content_inner"]/article/table') %>% html_table()
    product_information_booki <- select(data.frame(table), "X2")
    
    # get the image URL of book i
    img_url_part2 <- html_booki %>% html_elements("img") %>% html_attr("src")
    img_url_part2
    img_url_part2 <- img_url_part2 %>% substr(7, 10000) # choose random high value for end string
    img_url_part2
    img_url_booki <- paste("http://books.toscrape.com/", img_url_part2, sep="")
    img_url_booki
    
    # put in dataframe
    df$UPC[i] <-  product_information_booki[[1]][1]
    df$`Product Type`[i]  <-  product_information_booki[[1]][2]
    df$`Price (excl. tax)`[i]  <-  product_information_booki[[1]][3]
    df$`Price (incl. tax)`[i]  <-  product_information_booki[[1]][4]
    df$Tax[i]  <-  product_information_booki[[1]][5]
    df$Availability[i]  <-  product_information_booki[[1]][6]
    df$`Number of reviews`[i]  <-  product_information_booki[[1]][7]
    df$Description[i] <- description_booki
    df$Category[i] <- category_booki
    df$`URL book cover image`[i] <- img_url_booki
    
  }
  
  # Return the dataframe
  return(df)
}




#### Part 3: Function call (test) ####
df <- scrape_books("http://books.toscrape.com/")
view(df)

```

# Step 6: üßπ Tidy up the data

If time allows, take the opportunity to practice modifying the resulting data frame using the principles you've learned in earlier lectures. Make sure that the data types are appropriate and optimised for storage efficiency. For the sake of this exercise, we will assume that all columns are important and should be retained in the final data frame.
